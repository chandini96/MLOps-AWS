# MLOps Pipeline - AWS SageMaker

A production-ready, modular machine learning pipeline on AWS SageMaker with components for each stage of the ML lifecycle.

## ğŸ“¦ Components

### 1. **components/data_fetcher/** - Data Acquisition
- Fetches data from CSV files or URLs
- Docker containerized
- S3 compatible

### 2. **components/preprocess/** - Data Preprocessing  
- Handles missing values, encoding, normalization
- Docker containerized
- Input/output via S3

### 3. **components/train/** - Model Training (SageMaker Compatible)
- RandomForest model training
- S3 data input/output
- SageMaker container compatible

### 4. **components/evaluate/** - Model Evaluation
- Multiple metrics (accuracy, precision, recall, F1, ROC AUC)
- Visualization (confusion matrix, ROC curves)
- Feature importance

### 5. **components/model_registry/** - Model Versioning
- Model registration and versioning
- Metadata tracking
- Model promotion to production

## ğŸ—ï¸ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Fetch   â”‚ â†’ Fetches data from S3
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Preprocess   â”‚ â†’ Cleans and prepares data
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Train Model  â”‚ â†’ Trains RandomForest
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Evaluate     â”‚ â†’ Evaluates performance
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Register     â”‚ â†’ Registers in Model Registry
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- AWS Account with appropriate permissions
- Docker installed
- Python 3.9+
- AWS CLI configured

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Build Docker Images

```bash
# Build all components
make build

# Or build individually
make build-data        # Data fetcher
make build-preprocess  # Preprocessor
make build-train       # Trainer
make build-eval        # Evaluator
make build-registry    # Model registry
```

### 3. Test Components Locally

```bash
make test
```

### 4. Deploy to SageMaker

#### Option A: Using deploy script

```bash
export SAGEMAKER_ROLE="arn:aws:iam::ACCOUNT:role/SageMakerRole"
export S3_BUCKET="your-mlops-bucket"
export AWS_REGION="us-east-1"

# Build and push images, then deploy pipeline
bash deploy_pipeline.sh
```

#### Option B: Manual deployment

```bash
# 1. Update ECR_REGISTRY in pipeline.py with your ECR repository
export ECR_REGISTRY="YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com"

# 2. Deploy pipeline
python pipeline.py --deploy

# 3. Start execution
python pipeline.py --start
```

## ğŸ“‚ Project Structure

```
mlops-aws/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ data_fetcher/
â”‚   â”‚   â”œâ”€â”€ data_fetch.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocess/
â”‚   â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate/
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ model_registry/
â”‚       â”œâ”€â”€ model_registry.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ pipeline.py              # SageMaker pipeline definition
â”œâ”€â”€ deploy_pipeline.sh       # Automated deployment script
â”œâ”€â”€ docker-compose.yml       # Local testing with docker-compose
â”œâ”€â”€ Makefile                 # Build commands
â”œâ”€â”€ requirements.txt         # Single requirements file for all components
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

# Generated directories (not in git)
â”œâ”€â”€ data/                    # Local data storage
â”œâ”€â”€ models/                  # Trained models
â”œâ”€â”€ evaluation/              # Evaluation results
â””â”€â”€ registry/                # Model registry
```

## ğŸ”§ Configuration

### Environment Variables

```bash
# Required
export SAGEMAKER_ROLE="arn:aws:iam::ACCOUNT:role/SageMakerRole"
export S3_BUCKET="your-mlops-bucket"
export AWS_REGION="us-east-1"

# ECR Configuration (after pushing images)
export ECR_REGISTRY="YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com"

# Optional
export PIPELINE_NAME="mlops-pipeline"
```

### Pipeline Parameters

Edit `pipeline.py` to customize:
- Processing instance types
- Training instance types  
- S3 paths for input/output
- ECR image URIs

## ğŸ³ Docker Usage

### Local Testing with Docker Compose

```bash
# Start all services
docker-compose up

# Run in background
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Test Individual Components

```bash
# Data fetch
docker run -v $(pwd)/data:/app/data mlops-data-fetch

# Preprocess
docker run -v $(pwd)/data:/app/data \
           -v $(pwd)/processed_data:/app/processed_data mlops-preprocess

# Train (with AWS credentials)
docker run -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
           -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
           -e AWS_DEFAULT_REGION=$AWS_DEFAULT_REGION \
           mlops-train --s3_input_path s3://bucket/data.csv \
                       --s3_output_path s3://bucket/model.joblib

# Evaluate
docker run -v $(pwd)/models:/app/models \
           -v $(pwd)/evaluation:/app/evaluation mlops-evaluate

# Model Registry
docker run -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
           -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
           -v $(pwd)/registry:/app/registry mlops-registry
```

## â˜ï¸ AWS SageMaker Integration

### Pipeline Execution

The SageMaker pipeline automatically orchestrates all components:

1. **Data Fetch**: Downloads data from S3
2. **Preprocess**: Cleans and prepares training data
3. **Train**: Trains RandomForest on SageMaker
4. **Evaluate**: Computes evaluation metrics
5. **Register**: Registers model in SageMaker Model Registry

### Running in SageMaker

```python
import boto3

# Start pipeline execution
client = boto3.client('sagemaker', region_name='us-east-1')
client.start_pipeline_execution(
    PipelineName='MLOpsPipeline',
    PipelineParameters={
        'InputDataS3Uri': 's3://your-bucket/data/raw/dataset.csv',
        'ProcessingInstanceType': 'ml.m5.xlarge',
        'TrainingInstanceType': 'ml.m5.large',
        'ModelApprovalStatus': 'Approved'
    }
)
```

### View Pipeline in SageMaker Studio

1. Open SageMaker Studio
2. Navigate to **Pipelines** in the left sidebar
3. Select **MLOpsPipeline**
4. Click **Start execution**
5. Monitor execution status

### Using the CLI

```bash
# List pipeline executions
aws sagemaker list-pipeline-executions \
    --pipeline-name MLOpsPipeline \
    --region us-east-1

# Describe specific execution
aws sagemaker describe-pipeline-execution \
    --pipeline-execution-arn <execution-arn> \
    --region us-east-1
```

## ğŸ’» Python Usage

### Run Components Individually

```python
from components.data_fetcher import DataFetcher
from components.preprocess import DataPreprocessor
from components.evaluate import ModelEvaluator
from components.model_registry import ModelRegistry

# 1. Fetch data
fetcher = DataFetcher(data_dir="data")
df = fetcher.fetch_from_csv("your_data.csv")

# 2. Preprocess
preprocessor = DataPreprocessor()
df_processed = preprocessor.preprocess(df)

# 3. Save processed data
fetcher.save_data(df_processed, "processed_data.csv")

# 4. Evaluate (example)
evaluator = ModelEvaluator()
# ... evaluate model

# 5. Register model
registry = ModelRegistry()
# ... register model
```

## ğŸ§ª Testing

```bash
# Test all components
make test

# Test specific component
python -c "from components.data_fetcher import DataFetcher; print('âœ“ data_fetcher OK')"
python -c "from components.preprocess import DataPreprocessor; print('âœ“ preprocess OK')"
python -c "from components.evaluate import ModelEvaluator; print('âœ“ evaluate OK')"
python -c "from components.model_registry import ModelRegistry; print('âœ“ registry OK')"
```

## ğŸ“ Features

- âœ… **Modular Design**: Independent, reusable components in separate folders
- âœ… **Containerized**: Dockerized for consistent environments
- âœ… **AWS Native**: Built specifically for SageMaker
- âœ… **Production-Ready**: Logging, error handling, configuration management
- âœ… **Single Requirements**: One `requirements.txt` for entire project
- âœ… **Orchestrated**: Complete SageMaker Pipeline definition
- âœ… **Version Control**: Model registry with versioning and metadata
- âœ… **Scalable**: Each component can be scaled independently

## ğŸ”— Useful Commands

```bash
# Make commands
make help              # Show all available commands
make build             # Build all Docker images
make up                # Start with docker-compose
make logs              # View logs
make down              # Stop containers
make clean             # Clean Docker images and containers
make test              # Test all components

# Pipeline commands
python pipeline.py --deploy      # Deploy pipeline to SageMaker
python pipeline.py --start       # Start pipeline execution
```

## ğŸ“Š Monitoring

### View Pipeline Status

```bash
aws sagemaker list-pipeline-executions \
    --pipeline-name MLOpsPipeline \
    --max-results 10
```

### CloudWatch Logs

Each step logs to CloudWatch Logs:
- `/aws/sagemaker/ProcessingJobs` - Processing steps
- `/aws/sagemaker/TrainingJobs` - Training jobs
- `/aws/sagemaker/Models` - Model registry

## ğŸ”’ Security Best Practices

1. Use IAM roles with least privilege
2. Encrypt S3 buckets at rest
3. Use VPC endpoints for SageMaker
4. Rotate AWS credentials regularly
5. Enable CloudTrail for audit logging

## ğŸ“„ License

MIT License

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ”— Useful Links

- [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/)
- [SageMaker Pipelines Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines.html)
- [Docker Documentation](https://docs.docker.com/)
- [scikit-learn Documentation](https://scikit-learn.org/)
